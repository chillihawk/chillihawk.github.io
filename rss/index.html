<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Technology-on-Sea]]></title><description><![CDATA[Thoughts about technology from the South Coast.]]></description><link>/</link><generator>Ghost 0.6</generator><lastBuildDate>Thu, 05 Nov 2015 13:31:19 GMT</lastBuildDate><atom:link href="/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[What is the BIG deal about end user development?]]></title><description><![CDATA[<p>End User Development (EUD) also called User Developed Applications (UDA) is the practice of non-technologists or at least employees who are not in the technology department developing applications themselves and using those in a production setting.  </p>

<h6 id="somepointsofclarification">Some points of clarification</h6>

<ol>
<li>End User Computing (EUC) while sometimes mentioned in the same</li></ol>]]></description><link>/whats-the-big-deal-about-end-user-development/</link><guid isPermaLink="false">6caa3506-6e14-4e86-8121-c8cd2fff4500</guid><dc:creator><![CDATA[Mark Townsend]]></dc:creator><pubDate>Thu, 05 Nov 2015 12:24:13 GMT</pubDate><content:encoded><![CDATA[<p>End User Development (EUD) also called User Developed Applications (UDA) is the practice of non-technologists or at least employees who are not in the technology department developing applications themselves and using those in a production setting.  </p>

<h6 id="somepointsofclarification">Some points of clarification</h6>

<ol>
<li>End User Computing (EUC) while sometimes mentioned in the same context is something different.  I define this as the practice of end users consuming technology rather than creating it.  In my mind there is also an implication that the use of that technology is broadly inline with its expected use.  </li>
<li>I'm specifically discussing the creation of technology in this post as opposed to the use of an existing technology, for example: downloading and using a browser plugin that is non-standard for your organisation.  I include the latter practice under the broader umbrella of Non-standard Technology Use (NTU) to which many of the same arguments in this post apply but it's not under discussion here.  </li>
<li>I'm mostly discussing spreadsheet applications as that seems to be the predominant technology for end user development in the corporate space.  However batch files, shell files, PowerShell files, VBScript files, JavaScript files, Access databases, office macros are all available to be 'developed' by the end user on most corporate desktops. </li>
</ol>

<h4 id="whatsthelittledeal">What's the little deal?</h4>

<p>Many of you will be familiar with the issues that arise when non-experts develop applications and/ or when applications are developed in non-typical ways.  This is not an exhaustive list and I'd love to hear any others you can think of but here's a few:</p>

<ul>
<li>inexpert (probably) use of the language and tooling leads to wasted time</li>
<li>no independent testing leads to errors in production </li>
<li>no version control/ proliferation of old copies leads to wasted time and errors in production</li>
<li>no quality control leads to errors</li>
<li>no documentation leads to wasted time</li>
<li>no resiliency planning leads to wasted time when unexpected circumstances arise</li>
<li>opaque dependencies means applications depend on data/ connectivity/ user input/ accounts that is not obvious</li>
<li>'super glue' effect means applications get in to a corporate production environment and are very hard to remove</li>
<li>'tail wagging the dog' effect means processes form around the application and pretty soon you are 'opening the confirmation spreadsheet' rather than 'confirming the transaction'.  </li>
</ul>

<p>All of that basically boils down to <strong>errors</strong>, <strong>poor process</strong> and <strong>inefficiency</strong>.  That is all bad, I agree.  But it's not the really <strong>BIG</strong> deal.</p>

<h3 id="whatsthebigdeal">What's the BIG deal?</h3>

<p><img src="/assets/images/paymentProcessing.jpg" alt="">
At one point in my professional past I worked for an organisation that did payment processing.  They processed millions of transactions daily, all across the globe and had very robust and suitable systems for this (and I dare say larger) workloads.  They also had over <strong>4000</strong> user developed applications.  Presuming every one of those applications had a single core use case then the end users in this business thought there were an additional <strong>4000</strong> ways to do this business!</p>

<p><strong>That is the BIG deal!</strong>  </p>

<p>This is interesting for so many reasons and I'll discuss a few in the next couple of paragraphs.  Let me first state however that I didn't dig in to this 4000 so I cannot make any empirical statements about the complexity of these applications, the duplication in this population (although I know from  anecdotal evidence that it certainly wasn't 4000 copies of the same spreadsheet), or the current use.  That certainly would have been nice to have but I'll continue without it in a more general sense of a mature business with a non-trivial number of user developed applications.</p>

<hr>

<p>Let's go!</p>

<p>Firstly, the payment processing business is very competitive and is only profitable at scale therefore any organisation that is in it must have the systems to do it.  They've got the core use cases covered.  They've got the non-core use cases covered.  They are working on new use cases, at scale.  If there were 4000 use cases they hadn't covered they'd be out of business.  So those applications represent wasted effort.</p>

<p>Secondly, looking at this from the point of view of organisational dysfunction, end users are the ones in the trenches, doing the job/ using the systems day in and day out and they have requirements that aren't being met.  They have 4000 of them!  I know from first hand experience that end users are not knocking up spreadsheets just for the fun of it and that many End User Developers find it is a frustrating process but, that frustration must have been outweighed by the frustration of an unmet requirement.  4000 times.  I think it's reasonable to suggest this evidences a degree of the technology department being out of touch with its user base and/ or the business management failing to communicate effectively with the employees.</p>

<p>Thirdly, looking at this from the point of view of individual responsibility, how can an end user who understands the payment processing business think there are an additional valid 4000 things the organisation should be doing?  There aren't!  Now, granted, there's typically no global view, in other words the end user doesn't know that they are making the 4000th UDA but they must know they are not expert developers and consequently they are doing something potentially dangerous for the business.  It's certainly true that the negative aspects of end user development are more widely known now than they once were.  But very often it is easier to 'just bang out a spreadsheet' than to raise a requirement against a strategic system and argue for it's escalation and prompt fulfillment.  So users have been doing the easy thing and not the right thing.   </p>

<p>Finally, looking at this from the point of view of business agility, these applications are business quick sand.  For all the reasons stated in the <strong>little deal</strong> section earlier, these applications tend to be very brittle.  They very often cannot be run: by a different user; on a different machine; on a different date; in a different context or with different data.  They are almost never scale invariant (in other words they will break with more volume) and they are almost never defect free.  They are invisible at a global level but indispensable at a local level meaning they don't feature in planning and strategy but they are an impediment to execution of that strategy.  </p>

<hr>

<p>In summary, in this post I've deliberately ignored the positives of end user development and instead focussed on the negatives.  Of those bad aspects I acknowledge the importance of the poor SDLC (Software Development Life Cycle) practices but argue they are of secondary importance next to the issues of organisational dysfunction, communication, individual responsibility and business agility that the existence of large populations of user developed applications bring up in mature businesses.  As always, I'd be very interested in your comments.</p>

<p>Have fun!</p>]]></content:encoded></item><item><title><![CDATA[What are all the rows for in Excel?]]></title><description><![CDATA[<h3 id="alookatthefusecorpususingfnodejscouchdbandrpart1ofn">A look at the FUSE corpus using F#, nodejs, CouchDB and R.  Part 1 of n.</h3>

<p>Most spreadsheet researchers are familiar with the <a href="http://openscience.us/repo/spreadsheet/euses.html">EUSES Corpus</a><sup id="fnref:1"><a href="/what-are-all-the-rows-for/#fn:1" rel="footnote">1</a></sup> (4498 spreadsheets).  More recently (thanks to <a href="http://bit.ly/1NAOrK2">Felienne Hermans</a> and <a href="https://people.engr.ncsu.edu/ermurph3/">Emerson Murphy-Hill</a>), the <a href="http://figshare.com/articles/Enron_Spreadsheets_and_Emails/1221767">Enron Corpus</a><sup id="fnref:2"><a href="/what-are-all-the-rows-for/#fn:2" rel="footnote">2</a></sup> (15,770 spreadsheets), came to the attention of the</p>]]></description><link>/what-are-all-the-rows-for/</link><guid isPermaLink="false">b0ca6e05-7d40-4c86-8c49-d740a7581330</guid><dc:creator><![CDATA[Mark Townsend]]></dc:creator><pubDate>Wed, 19 Aug 2015 15:38:53 GMT</pubDate><content:encoded><![CDATA[<h3 id="alookatthefusecorpususingfnodejscouchdbandrpart1ofn">A look at the FUSE corpus using F#, nodejs, CouchDB and R.  Part 1 of n.</h3>

<p>Most spreadsheet researchers are familiar with the <a href="http://openscience.us/repo/spreadsheet/euses.html">EUSES Corpus</a><sup id="fnref:1"><a href="/what-are-all-the-rows-for/#fn:1" rel="footnote">1</a></sup> (4498 spreadsheets).  More recently (thanks to <a href="http://bit.ly/1NAOrK2">Felienne Hermans</a> and <a href="https://people.engr.ncsu.edu/ermurph3/">Emerson Murphy-Hill</a>), the <a href="http://figshare.com/articles/Enron_Spreadsheets_and_Emails/1221767">Enron Corpus</a><sup id="fnref:2"><a href="/what-are-all-the-rows-for/#fn:2" rel="footnote">2</a></sup> (15,770 spreadsheets), came to the attention of the wider community.  More recently still, the <a href="http://static.barik.net/fuse/">FUSE corpus</a><sup id="fnref:3"><a href="/what-are-all-the-rows-for/#fn:3" rel="footnote">3</a></sup>, has made some 249,376 spreadsheets available to everyone.  So, I decided to have a look at the use of the grid across all these workbooks.  </p>

<hr>

<h4 id="summary">Summary.</h4>

<p>The process I followed succeeded in analysing nearly 95% of the spreadsheets in the corpus.  The scatter plots below show the last row and last column produced by the analysis for each of the 236,226 workbooks I was able to analyse completely.  </p>

<p><img src="/assets/images/small_xy_log.jpg" alt="pre 2007 last row vs last col, log scale">
<img src="/assets/images/big_xy_log.jpg" alt="2007-2013 last row vs last col, log scale"></p>

<p>The analysis shows concentration of use of the grid is overwhelmingly in the upper left corner (in other words R1C1).  This is borne out by the summary statistics as well. <br>
<img src="/assets/images/summarystatistics.jpg" alt=""></p>

<p>The workbooks break down as follows:</p>

<p><img src="/assets/images/summarynumbers.jpg" alt=""></p>

<p>Once extreme workbooks are removed, distinguishing between pre-2007 sheets and the rest shows that generally Excel users are not taking advantage of the extra 983,040 rows nor extra 16,128 columns<sup id="fnref:4"><a href="/what-are-all-the-rows-for/#fn:4" rel="footnote">4</a></sup>.  In fact while there has been a more than thousand fold increase in the number of available cells there has only be an approximate doubling in the total number of cells used.   </p>

<p>Even including the extreme workbooks the average Excel 2007-2013 workbook in the analysis uses just less than 0.01% of the grid.  Looking at it another way, based on 0.01% occupancy rate, if Excel was Las Vegas, there'd only be 15 people staying tonight<sup id="fnref:5"><a href="/what-are-all-the-rows-for/#fn:5" rel="footnote">5</a></sup>.  </p>

<hr>

<p>As practicioners, we know this is true from our own experience.  We even advise and prefer parsimonious and organised use of the grid because it generally aids understanding of the workbook.  Using the grid for persisting data is contrary to best practice and no higher level function in Excel (non-trivial calculation, pivot, chart, print) involving that many (heterogenous) cells could be sufficiently performant (on a desktop machine) or even be guaranteed to work.  So, from the point of view of best practice it's a good thing that, on the whole, users aren't filling the grid.  </p>

<p><strong>So why are all the rows there?</strong>  </p>

<p>They are there for the edge case.  So that regardless of the size of your data, the grid will not hinder you.  In my view however it hinders the beginner.  Any piece of data can go in an effectively limitless number of locations.  When teaching I very commonly come across the question: <em>'where should I put 'X''</em>.  The extent of the grid demands organisation.  The extent of the grid permits effectively unbounded expansion of a model.  How many novice users arrange their spreadsheets in such a way that they can cope with the flexibility the grid permits?  Of course, the answer is, very few.  </p>

<p>This highlights how important it is to have a methodology when developing/ using spreadsheets.  If you are in the financial sector, traditionally heavy users of Excel, then there are a number of organisations like <a href="http://bit.ly/1K0lrLc">Operis</a>, <a href="http://bit.ly/1Jv98zZ">Corality</a> and <a href="http://bit.ly/1MT74Jd">F1F9</a> who offer training and even valuable free advice on best practice use of Excel.  But there is nothing to stop you or your organisation developing your own methodology, especially if a 'balance sheet style' doesn't suit your domain.  It can start simply with something like: <em>every cell with a number must be accompanied by a row header or column header explicitly stating the units of measure</em>, and evolve from there.  <strong>Getting a consistent approach to spreadsheet development/ use across your organisation will be worth the effort.</strong></p>

<p>Separately, I've written other posts about a <a href="http://bit.ly/xlnovice">novice mode</a> in Excel and I'd like to add to that in this post.  I think the novice mode should include the following:</p>

<ul>
<li>grey out anything below and to the right of the viewport (anchored at cell A1), to discourage (but not prevent) putting data there</li>
<li><em>'garbage collect'</em> (recursively) the furthest unused cells below and to the right of the viewport.  Garbage collection would mean if a cell contains no data, no formulas, is not part of a named range and is not in a print area then it should be greyed out and excluded from the <code>Worksheet.UsedRange</code>.  The used range must always be rectangular however, a jagged range is not permitted.</li>
</ul>

<p>I plan to continue analysis of the FUSE corpus with a focus on identifying aspects of spreadsheet use that could be improved to make life easier for Excel beginners.  So expect more posts on the idea of a novice mode and ways in which constraining the product can actually make it easier to use for most people.  I'd really appreciate any comments you'd like to share.</p>

<p>The remainder of this post outlines the steps I took to perform the analysis.</p>

<hr>

<h4 id="method">Method</h4>

<p>If you'd like to follow along with the method, I am referencing the information in this link: <a href="http://static.barik.net/fuse/">static.barik.net/fuse/</a>.</p>

<p>This analysis was performed on the current Fuse set which at the time of writing was 249,376 binaries extracted from the Common Crawl files from Winter 2013 through December 2014.  Fuse is a dynamic archive hence why I'm recording this fact.  </p>

<p>236,226 workbooks were successfully analyzed.   In order to perform this analysis I took the following steps:</p>

<ul>
<li>Downloaded the JSON metadata file (fuse-bin.analysis.dedup.poi-dec2014.json.gz)</li>
<li>Decompressed the file with <a href="http://www.7-zip.org/">7-zip</a> and used <a href="https://nodejs.org/">nodejs</a> to read it into a <a href="http://couchdb.apache.org/">CouchDB</a> instance.  <a href="https://www.mongodb.org/">MongoDB</a> is suggested and commands are provided to load it in to a MongoDB instance but it's not my cup of tea.  CouchDB worked fine in any case, although because I was doing it on my laptop I had to split the file and batch the inserts to get it to work.</li>
<li>FUSE does provide <a href="https://poi.apache.org/">POI</a> stats in the JSON, but a MapReduce on <code>doc.POI['countCOLUMNS']</code>, <code>doc.POI['countCOLUMN']</code>, <code>doc.POI['countROW']</code> and <code>doc.POI['countROWS']</code> seemed to show the majority of these counts at zero, so I decided to roll my own.</li>
<li>Conveniently FUSE provides all 249,376 binaries and an index on their site so I downloaded the index file (fuse-all.sha1.sorted-dec2014.txt) and the archive (fuse-binaries-dec2014.tar.gz).</li>
<li>Decompressed the archive using <a href="http://www.7-zip.org/">7-zip</a>.  It is 21.3GB fully expanded.</li>
<li>I then iterated through the collection using F# and COM Interop to discover the last non-empty row and last non-empty column on each worksheet in each workbook.  Then per workbook I took the maximum last used row from all sheets in the book and separately the maximum last used column from all sheets to produce a blended maximum used range for the workbook.  In other words, using the example below, if the maximum used range in Sheet1 was A1:T10 and the maximum used range in Sheet2 was A1:E15 the workbook maximum used range was A1:T15.</li>
</ul>

<p><img src="/assets/images/workbookmaxrange-1.jpg" alt="How to choose the maximum used range"></p>

<div class="callout"><p>An aside here.  There are plenty of other technologies available to read spreadsheets and I looked at and dismissed the following: <a href="http://www.gemboxsoftware.com/spreadsheet/overview">Gembox</a> (because the free version only lets you read 150 rows per sheet and 5 sheets per workbook), <a href="http://fsprojects.github.io/ExcelProvider/">ExcelProvider</a> (because it looks at the file extension to determine whether to open a binary stream or an xml stream and the FUSE file names don't have file extensions), <a href="https://npoi.codeplex.com/">NPOI</a> (this is the .NET port of POI so it was appealing to use similar technology to the FUSE guys but I just didn't grok it immediately so maybe I'll come back to it) and <a href="https://exceldatareader.codeplex.com/">ExcelDataReader</a> (I did a partial implementation using this but there ended up being a lot of files it couldn't open, the crux of the issue being not having a good way to determine whether to open a binary stream - for xls - or an xml stream - for xlsx).  So I settled on Office.Interop.  Which, is not without its own issues - especially around tightly managing your RCW references (here's a good <a href="http://stackoverflow.com/questions/4591681/rcw-reference-counting-when-using-com-interop-in-c-sharp">stackoverflow post</a> on the subject) - but FUSE notes some 28,616 workbooks unreadable by their analysis tools as opposed to my 13,150.  Many of the COM errors came from password protected workbooks and the sudden death of the RPC server, which I couldn't get to the bottom of.  One further point worth noting is that COM introduces a dependency on Windows which restricts your VM options if you are pushing this analysis up into the cloud.  A definite next step for me so whether or not I continue with COM will remain to be seen.  
</p></div>  

<ul>
<li>The technique I used for finding the last used column was:</li>
</ul>

<pre><code class="language-f#">let cellsColl = sheet.Cells  
let firstCell = cellsColl.[1,1] :?&gt; Range  
let mutable lastCell = cellsColl.[1,1] :?&gt; Range  
let mutable lastCol = -1  
try  
  try
    lastCell &lt;- cellsColl.Find("*", firstCell, Excel.XlFindLookIn.xlFormulas, Excel.XlLookAt.xlPart, Excel.XlSearchOrder.xlByColumns, Excel.XlSearchDirection.xlPrevious, Type.Missing, Type.Missing, Type.Missing)
            if(not(obj.ReferenceEquals(lastCol, null))) then lastCol &lt;- lastCell.Column
        with
            | _ as ex -&gt; printfn "%s has thrown an error: %s" sheet.Name ex.Message
finally  
  // clean up com references    
</code></pre>

<p>Then change <code>Excel.XlSearchOrder</code> to <code>Excel.XlSearchOrder.xlByRows</code> to find the last used row.  I have assumed content in all sheets starts in cell A1 rather than attempt to resolve the upper left corner of the used range aswell as the bottom right.  I think this is a reasonable assumption.</p>

<ul>
<li>Having got the results I then used nodejs to update the JSON records in CouchDB.  Once again due to the resource constraints on my machine this operation needed to be batched.</li>
<li>Finally I wrote another MapReduce to get the results back out in csv and loaded this content in to <a href="https://www.r-project.org/">R</a>.  </li>
<li>I split the data in to pre and post Excel 2007 groups then produced the scatter plots for the last row vs the last column.  I used the same log scales for both pre and post Excel 2007 to highlight the similarity in the distributions.</li>
</ul>

<p>In terms of improving the method it's worth noting that COM is not fast.  It also introduces a dependency on Windows and Excel.  Also, the programming is just plain ugly.  To remove the need for COM a reliable method for determining whether the file is a binary file or xml file would need to be developed and then the ExcelDataReader could be used.  NPOI is also worth further investigation.  I will look at these two technologies next before pushing the analysis up in to the cloud. </p>

<p>Please comment if you've got any questions I can help with.</p>

<p>Have fun!</p>

<hr>

<h6 id="footnotes">Footnotes:</h6>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Marc Fisher II and Gregg Rothermel. The EUSES Spreadsheet Corpus: A shared resource for supporting experimentation with spreadsheet dependability mechanisms. In Proceedings of the 1st Workshop on End-User Software Engineering, pages 47-51, St. Louis, MO, USA, May 2005 <a href="/what-are-all-the-rows-for/#fnref:1" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:2"><p>Felienne Hermans and Emerson Murphy-Hill. Enron's Spreadsheets and Related Emails: A Dataset and Analysis.  37th International Conference on Software Engineering, ICSE 2015 <a href="/what-are-all-the-rows-for/#fnref:2" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:3"><p>Titus Barik, Kevin Lubick, Justin Smith, John Slankas, and Emerson Murphy-Hill. "<a href="http://go.barik.net/msr2015">Fuse: A Reproducible, Extendable, Internet-scale Corpus of Spreadsheets</a>." In: Proceedings of the <a href="http://2015.msrconf.org/">12th Working Conference on Mining Software Repositories</a> (Data Showcase), Florence, Italy, 2015. <a href="/what-are-all-the-rows-for/#fnref:3" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:4"><p>In section V of the FUSE paper a limitation of the Common Crawl which means it can only store binary files no greater than 1MB is highlighted so I'm aware that this analysis excludes really big Excel files and that they could conceivably skew the grid usage down and to the right.  But the paper goes on to state that such files only make up low single digit percentages in the other available corpora so my contention is that it wouldn't make a very big difference to this analysis.    <a href="/what-are-all-the-rows-for/#fnref:4" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:5"><p>150,544 rooms as at 03/2015 according to the <a href="http://www.lvcva.com/includes/content/images/media/docs/2014-Vegas-FAQs.pdf">Las Vegas Convention and Visitor Authority</a> <a href="/what-are-all-the-rows-for/#fnref:5" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:6"><p>An extreme workbook is defined as one where data is reported either in row  1,048,576 or column 16,384.  These are removed from the workbook summary only for the purpose of calculating the blended average cell usage.  They remain in the scatter plots and statistical analysis. <a href="/what-are-all-the-rows-for/#fnref:6" title="return to article">↩</a></p></li></ol></div>]]></content:encoded></item><item><title><![CDATA[Greedy formulas and layout errors, the case for a novice mode in Excel.]]></title><description><![CDATA[<p><img src="/assets/images/errorcheckonvlookup1.JPG" alt="">
What you are looking at is an Excel 2013 worksheet containing a VLOOKUP formula written <em>slightly</em> incorrectly, giving <strong><em>completely</em></strong> the wrong result.  What you can also see is Excel giving two indications that something might be wrong.  However if you put "background error check excel" in to Google you will</p>]]></description><link>/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/</link><guid isPermaLink="false">dc89fcf6-8880-45a7-a729-a8fdccc568b8</guid><dc:creator><![CDATA[Mark Townsend]]></dc:creator><pubDate>Tue, 07 Jul 2015 19:42:00 GMT</pubDate><content:encoded><![CDATA[<p><img src="/assets/images/errorcheckonvlookup1.JPG" alt="">
What you are looking at is an Excel 2013 worksheet containing a VLOOKUP formula written <em>slightly</em> incorrectly, giving <strong><em>completely</em></strong> the wrong result.  What you can also see is Excel giving two indications that something might be wrong.  However if you put "background error check excel" in to Google you will find as many links telling you how to <strong>turn it off</strong> as how to use it properly.<sup id="fnref:1"><a href="/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/#fn:1" rel="footnote">1</a></sup>  As I'll demonstrate, using it to trap this particular error scenario is not straightforward. If you'd like to follow along the workbooks are in the 2013 folder in my <a href="https://github.com/markstownsend/excel-badpractice">Github repo</a>.  Please star it if it helps you.</p>

<p>I've spent a lot of time over the last several years in big corporates teaching people how to use Excel.  In that time I've seen plenty of VLOOKUPs that have been entered like this.  What's more I've heard the use of this particular formula syntax (with the second argument specified as columns, this is the <strong>greedy</strong> in the blog title, and the deliberate absence of the final argument) advocated at a peer to peer level, in other words, colleagues learning from their colleagues.  </p>

<p>In the rest of this post I'll examine what's going wrong, how Excel is trying to tell us and where that explanation requires some intuitive leaps which are most likely beyond the understanding of novice spreadsheet users.</p>

<hr>

<p>The following image is the function wizard dialogue for the particular formula in question and I draw your attention to the elements I've labelled 1 and 2.</p>

<p><img src="/assets/images/functionwizarddialogueonvlookup1.jpg" alt="Function Wizard"></p>

<ol>
<li><p>The obliquely named <strong>Table<em>_</em>array</strong> is the range, in the leftmost column of which you will attempt to match your <strong>Lookup<em>_</em>value</strong>.  Now this is a legitimate range, (it's all of column B, C, D and E), but in the case of this workbook there is data further down in those columns, out of sight and impacting the formula result (this is the <strong>layout error</strong> in the blog title, there are 1,048,576 rows after all).  The justification I've heard for using the range <code>B:E</code> rather than <code>$B$3:$E$6</code> is because it is quicker to select and because using that syntax you won't miss any data or have to update your formula should more rows be added to the <strong>Table<em>_</em>array</strong>.  Well, given a preference for use of the mouse over the keyboard, not uncommon in novice users, then I agree it probably is quicker to select.  And you certainly won't miss any data but the danger is you will include data that you didn't intend to include.  Finally, there are better ways to have a formula that will automatically update when new rows are added, <a href="https://support.office.com/en-za/article/Create-an-Excel-table-in-a-worksheet-e81aa349-b006-4f8a-9806-5af9df0ac664">Tables</a> come to mind immediately.</p></li>
<li><p>The <em>even more</em> obliquely named Range_lookup is displayed in standard font (as opposed to bold), indicating that it is optional.  It hasn't been entered and the behaviour is as expected<sup id="fnref:2"><a href="/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/#fn:2" rel="footnote">2</a></sup>, an approximate match.  Now the justification I've heard for this is, circularly: <em>'it's optional, you don't need to enter it'</em>.  I don't want to examine approximate matching on text values, there are valid cases but they are edge cases.  I use and teach the heuristic that when matching text values this final argument should <strong>always</strong> be specified as <code>FALSE</code> indicating an exact match is required.  That's the case in the example under discussion here.  </p></li>
</ol>

<p>So, in summary, the final argument is wrong for matching on text values and the second argument should be more parsimoniously stated.  But what does the error checking say?</p>

<p><img src="/assets/images/errorcheckmessage1.jpg" alt=""></p>

<p>Quite honestly, that doesn't seem like a big deal to me.  I've also run the INQUIRE workbook analysis on this book and in the report the following information is presented.  I've drawn out the salient points for you because frankly, it isn't obvious. <br>
<img src="/assets/images/worksheetanalysisonvlookup1.jpg" alt="">
Once again we're referencing blank cells.  Once again, no big deal.  We also have a formula referencing text cells.  Well that's what we wanted we're matching on the widget name after all so, no big deal.  Granted, 384 blank referenced cells jumps off the page a bit but that's just stating the first point the other way round so, no big deal.  But, the error is egregious!  We're reporting the revenue for the full year 2014 sales of Widget A instead of the part year 2015 sales.  Where is the warning:</p><h1>Danger, Will Robinson!</h1>  <p></p>

<p>Instead it's a meeker:  </p>

<h5>Danger?  Will Robinson.</h5>

<p>Taking the text 'error' first and drilling in to the appropriate sheet <code>[Text Cell Refs]</code> on the analysis workbook we should interpret it like this: the formula is a lookup and it is working on cells containing text.  We have to link that in our mental Excel heuristics dictionary to the rule of thumb: <strong>matching on text values must be exact</strong>.  Having made that association then we know we must make a correction to the formula at least as follows, from <code>=VLOOKUP(I3,B:E,4)</code> to <code>=VLOOKUP(I3,B:E,4,FALSE)</code>. Now, this will be enough to fix the error but let's continue.</p>

<p>Taking the two blank 'errors' next and drilling in to the appropriate sheets <code>[Blank Cell Refs]</code> and <code>[Blank Referenced Cells]</code> on the analysis workbook we should interpret it like this:  the formula is a lookup and it is looking up in a range that contains blank cells.  Now in and of itself that is not a problem for a lookup (performance notwithstanding).  In fact this analysis is a very indirect way of zeroing in on the real issue.  But, at this point we should consult our mental Excel heuristics dictionary for the rule of thumb: <strong>parsimony over greed</strong> when consuming arguments in a function.  If we're not swayed by this we should, at a minimum, traverse the entirety of the left-most column (<code>B</code>)<sup id="fnref:3"><a href="/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/#fn:3" rel="footnote">3</a></sup> of the Table<em>_</em>array (that's the column where the Lookup_value is being matched) and in so doing we would discover the data for 2014.  We could then move the 2014 data somewhere else which would fix the error, in which case we could leave the formula unchanged and ignore or dismiss the error.  Hence why I stated earlier that this analysis is only an indirect way of identifying the issue.  Instead however, we will make a correction to the formula at least as follows, from <code>=VLOOKUP(I3,B:E,4)</code> to <code>=VLOOKUP(I3,B3:E6,4)</code>.  That will also be enough to fix the error.</p>

<p>Finally we should make both corrections so the formula becomes <code>=VLOOKUP(I3,B3:E6,4,FALSE)</code> or better yet <code>=VLOOKUP(I3,$B$3:$E$6,4,FALSE)</code>.  We should also move the 2014 data, probably on to its own tab.  </p>

<hr>

<p>I think you'll agree that none of that was particularly obvious.  That's because that use of the formula is not wrong, <em>per se</em>, it's only ill-advised.  Therefore it's difficult to catch it.  Were there a syntax error Excel might catch it as a <code>#REF!</code> or it might pop up a warning and prevent the formula from being calculated.  But it can't tell you this is outright wrong because it's not.  But it is!</p>

<p>In order to address this, in my opinion, Excel should have a novice mode.  I will expand on this in future blog posts but for the time being, that novice mode should have the following functionality:</p>

<ul>
<li>forbid optional arguments in vlookup (I'll look in to whether this could be extended to all built in functions or a popular subset)</li>
<li>forbid all column only range references in all functions (in other words references like <code>B:E</code>)</li>
</ul>

<p>I know I'm not the only one with a view on the Excel flexibility vs control debate.  There are parties trying to address this by bringing software development best practice to spreadsheet development with unit/ integration testing and peer review among other things.  There are companies selling rules based automated monitoring.  There is more training available than you can possibly imagine.  And there's also error checking in the box.  But when a novice opens Book1.xlsx and enters some data and a formula, that's spreadsheet use, not development.  And there's currently nothing to stop them doing it wrong.  I'd very much appreciate any comments you would like to share on the idea of a novice mode. In the near future I'll write some more posts further examining this.</p>

<p>Have fun!</p>

<hr>

<p>Footnotes:  </p>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>I'm aware how unscientific this statement is given the tailored search results from Google nevertheless the point that there are a lot of links advising how to disable error checking is valid.  <a href="https://www.google.com/#q=background+error+check+excel">Try it for yourself</a> and see what you get. <a href="/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/#fnref:1" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:2"><p>As an aside I think there is a case to be made for an apparent inconsistency in not entering the final argument.  Spot the subtle difference in the formulas below but the big difference in the formula result.  I'll follow up on this in a future post.<img src="/assets/images/optionalfinalargumentinvlookup1.jpg" alt="" title=""> <a href="/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/#fnref:2" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:3"><p>Use the key combination CTRL + DOWN ARROW to quickly skip down the column from data region to data region.   <a href="/greedy-formulas-and-layout-errors-the-case-for-a-novice-mode-in-excel/#fnref:3" title="return to article">↩</a></p></li></ol></div>]]></content:encoded></item><item><title><![CDATA[Hosting a Ghost blog on Github for free!]]></title><description><![CDATA[How to host a Ghost blog on Github Pages for free.]]></description><link>/hosting-a-ghost-blog-on-github-for-free/</link><guid isPermaLink="false">7eb18528-b5ce-486a-9c3c-b30a56f666fa</guid><dc:creator><![CDATA[Mark Townsend]]></dc:creator><pubDate>Wed, 01 Jul 2015 07:56:23 GMT</pubDate><content:encoded><![CDATA[<p><img src="/assets/images/ghostplusgithubequalsaving.jpg" alt=""></p>

<p>What you are looking at is a blog post written on the <a href="https://ghost.org/">Ghost</a> blogging application then turned into static html and pushed in to a <a href="https://github.com/">Github</a> repo.</p>

<p>Sounds like a lot of hassle, why bother?  Well, because it will save you money.</p>

<p>In order to edit the blog I run <a href="https://ghost.org/">Ghost</a> v0.6.4 locally in <a href="https://nodejs.org/">node</a> v0.10.9 on my Windows machine.  On changing content/ theme or adding/ editing a post I use <a href="https://github.com/leftofnull/buster.git">Buster</a> to transpile the dynamic content from the running <a href="https://ghost.org/">Ghost</a> instance in to interim static html pages.  I then run some post-processing steps with <a href="http://gulpjs.com/">Gulp</a> to produce the final version of the static html and assets (css, js, images etc.).  Finally I push those contents up to my <a href="https://github.com/markstownsend/markstownsend.github.io">GitHub</a> repo.</p>

<p>I've taken the additional step of having my own custom url so I paid for the registration (USD14) and I pay monthly for the DNS using <a href="https://dnsimple.com">DNSimple</a> which is USD0.80 on a maxed out (10 domains for USD8 per month) legacy Silver plan.  So it's not quite <strong>free</strong> but 80 cents a month running cost for a blog takes some beating!</p>

<p>If you are going to take this route to host your blog then the following two links are invaluable and <a href="https://disqus.com/by/arcond/">Alexander Kahoun</a> is a legend.</p>

<ol>
<li><a href="http://leftofnull.com/2014/02/07/using-github-pages-with-ghost-and-buster-on-windows-part-1/index.html">Using GitHub Pages with Ghost and Buster on Windows (part 1)</a>  </li>
<li><a href="http://leftofnull.com/2014/02/24/using-github-pages-with-ghost-and-buster-on-windows-part-2/">Using GitHub Pages with Ghost and Buster on Windows (part 2)</a></li>
</ol>

<p>My own modest contribution to the process can be found in the README on my <a href="https://github.com/markstownsend/ghost-builder">ghost-builder repo</a> which you are welcome to have.  Please star it if it helps you.</p>

<p>It's further worth understanding the limitations of <a href="https://github.com/">Github</a> pages for hosting and why it's necessary to take all these steps.  Fundamentally <a href="https://pages.github.com/">Github Pages</a> will only serve static content.  That means only exactly the files hosted on the server.  It will not serve content from an app like <a href="https://ghost.org/">Ghost</a>.  That's why you aren't hosting <a href="https://ghost.org/">Ghost</a> on <a href="https://github.com/">Github</a> but rather the output from the <a href="https://ghost.org/">Ghost</a> application running on your local machine.  You are using <a href="https://ghost.org/">Ghost</a> because it is a beautiful way to blog and it is fully featured running locally.  But you could conceivably use another blogging platform and do the same sort of process to get it to emit static content which you then push in to your <a href="https://github.com/markstownsend">Github repo</a> to display using <a href="https://pages.github.com/">Github Pages</a>.  Phew!  Get it?  If not put a comment below and I'll try to help you out.  </p>

<p>Have fun!</p>]]></content:encoded></item></channel></rss>sing <a href="https://ghost.org/">Ghost</a> because it is a beautiful way to blog and it is fully featured running locally.  But you could conceivably use another blogging platform and do the same sort of process to get it to emit static content which you then push in to your <a href="https://github.com/markstownsend">Github repo</a> to display using <a href="https://pages.github.com/">Github Pages</a>.  Phew!  Get it?  If not put a comment below and I'll try to help you out.  </p>

<p>Have fun!</p>]]></content:encoded></item></channel></rss>